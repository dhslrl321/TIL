<<<<<<< HEAD
[학습 테스트로 배워보는 kafka] 5. 학습 테스트로 카프카 컨슈머(kafka consumer)와 offset 알아보기
=======
---

> 해당 시리즈에서 제공하는 모든 소스코드는 [github repository](https://github.com/my-research/kafka) 에서 제공됩니다. 자세한 코드와 테스트 케이스는 github repository 에서 확인해주세요.

[##_Image|kage@IlFlY/btstfmH4M7E/EWNC3VTHtHNX9FPKfWbsJ0/img.png|CDM|1.3|{"originWidth":966,"originHeight":486,"style":"alignCenter","width":538,"height":271}_##]

이번 [학습 테스트로 배워보는 kafka] 시리즈는 아래 **순서대로 챕터가 구성**되고, 시리즈 외로 kafka 관련하여 **더욱 많은 학습 정보**는 [kafka 심화 세션](https://wonit.tistory.com/category/%F0%9F%94%AC%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/-%20Event-Driven-Architecture) 에서 확인할 수 있습니다.

### 시리즈 목차

1. [시리즈를 시작하며](https://wonit.tistory.com/662) 
2. [kafka 빠르게 훑어보고 아는체하기](https://wonit.tistory.com/655) 
3. [kafka 컨셉과 용어 정리](https://wonit.tistory.com/656) 
4. [학습테스트 준비하기](https://wonit.tistory.com/657) 
5. [학습 테스트로 kafka producer 알아보기](https://wonit.tistory.com/658) 
6. [학습 테스트로 kafka consumer 알아보기](https://wonit.tistory.com/659) <-- **현재 글**
7. [학습 테스트로 partitioning 알아보기](https://wonit.tistory.com/660)
8. [학습 테스트로 consumer group 과 rebalancing 알아보기](https://wonit.tistory.com/661)

_학습의 단계별 순서로 목차가 구성되어있으므로 선행되어야 하는 챕터가 존재합니다_

---
>>>>>>> 15db0211 (commit)

---

> 해당 시리즈에서 제공하는 모든 소스코드는 [github repository](https://github.com/my-research/kafka) 에서 제공됩니다. 자세한 코드와 테스트 케이스는 해당 링크에서 확인해주세요.

[##_Image|kage@IlFlY/btstfmH4M7E/EWNC3VTHtHNX9FPKfWbsJ0/img.png|CDM|1.3|{"originWidth":966,"originHeight":486,"style":"alignCenter","width":538,"height":271}_##]

이번 [학습 테스트로 배워보는 kafka] 시리즈는 아래 **순서대로 챕터가 구성**되고, 시리즈 외로 kafka 관련하여 **더욱 많은 학습 정보**는 [kafka 심화 세션](https://wonit.tistory.com/category/%F0%9F%94%AC%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/-%20Event-Driven-Architecture) 에서 확인할 수 있습니다.

### 시리즈 목차

1. [시리즈를 시작하며](#)
2. [kafka 빠르게 훑어보고 아는체하기](#) <-- **현재 글**
3. [kafka 컨셉과 용어 정리](#)
4. [학습테스트 준비하기](#)
5. [학습 테스트로 kafka producer 알아보기](#)
6. [학습 테스트로 kafka consumer 알아보기](#)
7. [학습 테스트로 partitioning 알아보기](#)
8. [학습 테스트로 consumer group 과 rebalancing 알아보기](#)
9. [시리즈를 마치며](#)

_학습의 단계별 순서로 목차가 구성되어있으므로 선행되어야 하는 챕터가 존재합니다_

---

앞서 우리는 메시지를 카프카 브로커로 발행하는 **producer 의 java client** 에 대해서 알아보았다.

이번 시간에는 카프카 브로커에 저장된 메시지들을 **소비(consume) 하는 consumer java client** 인 `KafkaConsumer` 에 대해서 학습테스트를 통해 알아볼 예정이다

# KafkaConsumer

[##_Image|kage@b0BFuJ/btstqBjmRcn/P8VTsEapd03myrqZvt7B91/img.png|CDM|1.3|{"originWidth":3246,"originHeight":1834,"style":"alignCenter"}_##]

`KafkaConsumer` 클래스는 지난 Producer 와 반대의 일을 수행한다.

producer 가 브로커로 메시지를 저장했다면, consumer 가 브로커에 저장된 메시지를 읽는 것이다.

## KafkaConsumer 인스턴스 생성하기

생성자의 parameter 로 전달된 key-value 형태의 properties 를 통해서 client configuration 을 등록한다

[##_Image|kage@yPA4e/btsrBmpCwoo/Sxh7mcWyaRMIxw8mQO1Tx1/img.png|CDM|1.3|{"originWidth":1310,"originHeight":552,"style":"alignCenter","width":772,"height":325}_##]

해당 configuration 의 자세한 사항과 특정 설정들은 [confluent.io/kafka config](https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html) 에서 확인할 수 있다

앞선 프로듀서 테스트와 마찬가지로 여러 테스트에서 사용할 수 있도록 Test Hepler 클래스를 통해 쉽게 컨슈머 인스턴스를 생성할 수 있도록 분리하였다

```java
public class KafkaConsumerTestHelper {

  public static KafkaConsumer<String, String> simpleConsumer() {
    Map<String, Object> props = Map.of(
            "bootstrap.servers", "localhost:9092",
            "group.id", "my-consumer",
            "enable.auto.commit", "true",
            "auto.offset.reset", "earliest",
            // kafka 로 소비할 message 의 s/d 클래스 설정
            "key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer",
            "value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"
    );

    return new KafkaConsumer<>(props);
  }
}
```

여기서 눈여겨볼 설정은 `enable.auto.commit` 과 `auto.offset.reset` 인데 이는 추후 컨슈머의 commit 과 offset 파트에서 다시 이야기를 하도록 하겠다. (stay tuned!!)

## 테스트 1. KafkaConsumer 를 통한 메시지 소비 테스트

이번 테스트에서는 발행된 메시지를 KafkaConsumer 를 통해 소비해보도록 하겠다

[##_Image|kage@bRNao2/btsrDIysfeE/c1CGoEuAlxanjfnAo3uMGk/img.png|CDM|1.3|{"originWidth":2232,"originHeight":1336,"style":"alignCenter"}_##]

### 테스트 분석

- 주석 1.
  - 토픽에 메시지를 발행한다
  - consumer 테스트를 쉽게 하기 위해 테스트용 토픽에 메시지를 produce 하는 Helper 클래스가 사용되었다. 자세한 코드는 [github](https://github.com/my-research/kafka/blob/82ea1c688f5b551ce7840c200c650aeee12f9b12/kafka-test-supports/src/main/java/com/github/support/helper/KafkaConsumerTestHelper.java) 에서 확인할 수 있다
- 주석 2.
  - 카프카 컨슈머 인스턴스를 특정 토픽에 subscribe 시킨다.
  - 카프카 컨슈머 인스턴스는 한번에 여러개의 토픽에 subscribe 할 수 있다
- 주석 3.
  - subscribe 가 되었다고 해서 레코드를 계속해서 consume 하는것이 아니다.
  - 실제로 이벤트 로그를 fetch 하는 일은 `poll()` 메서드에서 수행된다
  - 결과로 ConsumerRecords 를 반환한다. 여러개의 토픽에 대해 consume 이 가능하기 때문에 record 자체도 여러개다.
  - timeout 에 대한 Duration 을 2초로 지정한다
- 주석 4.
  - 특정 토픽에 메시지가 잘 소비되었는지 검증하는 단언문이다. 역시 헬퍼 클래스이며 자세한 코드는 [github](https://github.com/my-research/kafka/blob/73d72a7bf4408b4e87f4d735e5c18ec1886a20a5/kafka-test-supports/src/main/java/com/github/support/assertions/KafkaAssertions.java#L11) 에서 확인할 수 있다

## test 2. 컨슈머의 multi topic consume 테스트

앞선 테스트에서 `subscribe()` 메서드를 통해서 여러개의 토픽에 대해 소비를 할 수 있다고 하였다.

실제로 테스트해보자

[##_Image|kage@d7AUbC/btsrDIkVPmw/GvalEdgrK9ySFxg7TEZI61/img.png|CDM|1.3|{"originWidth":2092,"originHeight":1570,"style":"alignCenter"}_##]

### 테스트 분석

- 주석 1.
  - 서로 다른 topic 에 ("my-topic-1", "my-topic-2") 메시지를 발행한다.
- 주석 2.
  - 하나의 컨슈머가 두개의 토픽을 subscribe 한다
- 주석 3.
  - 두개의 토픽 메시지를 소비하는 것을 확인할 수 있다

# kafka 의 offset 과 commit

카프카라고 한다면 아주 중요한 특성 하나가 존재한다.

#### 카프카는 다른 일반적인 메시지 큐와 **다르게** 메시지를 소비하더라도 해당 **메시지를 삭제하지 않는다.**

[AWS 의 SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html) 처럼 많은 상용 MQ 들을 보면, 메시지를 소비하고 `ack` 라는 과정을 통해 **메시지를 pop** 하여 다음 **메시지를 큐의 제일 끝단에 위치시키는게** 일반적이다.

하지만 kafka 는 그렇지 않다.

kafka 는 **offset** 을 통해 브로커 토픽에 저장된 메시지를 어디까지 읽었는지 확인하고 offset 을 계속해서 옮겨준다.

<<<<<<< HEAD
그래서 언제든지, 다른 컨슈머가 붙더라도 topic 에 대한 메시지를 **replay 할 수 있다**.
=======
#### 카프카는 로그의 **append only** 와 **seeking by offset** 특성을 따르는 `log based architecture` 를 지향한다.
>>>>>>> 15db0211 (commit)

이 컨셉이 바로 다른 메시지 미들웨어들과 카프카가 구분되는 중요한 이유라고 볼 수 있다.

# offset 과 commit

kafka 에서 producer 가 토픽에 저장한 메시지들은 linear 하게 쌓인다.

그럼 메시지들은 각각의 위치 정보(offset)을 갖게 되는데, 카프카 컨슈머는 이 위치 정보(offset)를 통해서 **어디까지 메시지를 소비하였는지**알 수 있다.

이러한 위치 정보(offset)는 kafka 내부의 `__consumer_offsets` 라는 특별한 토픽에 저장한다.

그리고 컨슈머가 메시지를 소비하고 나면 본인이 처리한 위치 정보(offset)를 업데이트 (commit) 한다.

commit 이라는 과정을 거치고 나면 다음 poll 타임에 새로 update 된 offset 부터 일정량만큼 메시지를 소비하게 되고, 앞선 과정을 반복하게 된다.

[##_Image|kage@GjFGb/btstsND3ZnL/7ljdu5m5xsUMT8mzTvZFM0/img.png|CDM|1.3|{"originWidth":1475,"originHeight":774,"style":"alignCenter","width":1106,"height":580}_##]

#### 왜 이렇게 offset 과 commit 이라는 개념을 만들었을까?

offset 과 commit 을 이용한다면 두가지의 큰 장점이 있다.

1. 메시지 replay
2. 안정성

kafka 가 다른 MQ 와 다른 이유는 message 를 pop 을 하지 않기 때문이라고 했다.

message 를 pop 하지 않고 메시지 중복을 막기 위해서는 offset 과 같은 위치정보가 필요하다.

그래야 특정 컨슈머가 이미 소비한 메시지에 대해서 중복해서 소비하지 않도록 한다.

이렇게 offset 을 저장하고 관리하면서 장애 상황에 대해 더 안정적으로 운영할 수 있게 된다.

장애가 발생하고 장애가 복구되었다면 마지막으로 commit 한 offset 부터 다시 읽으면 되기 때문이다

# commit 의 종류 2가지

이러한 commit 은 자동/수동 commit 이 존재한다.

### auto commit

auto commit 은 메시지를 가져올 때마다 commit 을 하는 방법이다.

일정 주기마다 혹은 메시지를 소비한 후에 자동으로 offset 을 commit 하는 방법이다.

`KafkaConsumer` 에서는 `poll()` 을 호출할 때 가장 마지막 offset 을 commit 한다.

이 방법의 경우, 오프셋을 관리하는 가장 편리한 방법으로 알려져있다.

하지만 메시지의 중복 처리 문제가 발생할 수 있는데, 만약 메시지를 소비했지만 poll 을 호출하기 전에 장애가 발생한다면 offset update 가 되지 않았으므로 메시지가 중복해서 소비된다.

### manual commit

manual commit 은 메시지 처리가 완료될 때까지 commit 하지 않는다.

일반적인 acknowledgement 와 동일하게 동작한다.

컨슈머가 메시지를 처리한 뒤, 명시적으로 처리된 offset 을 commit 하는 방식이다

이 방법의 경우, 메시지 소비 자체는 느릴 수 있으나 메시지 처리의 정확성과 무손실을 보장하는 방법이다.

이제 테스틀 통해 알아보자

## test 3. auto commit test

자동 커밋을 테스트해보자

[##_Image|kage@p1UU0/btstlg8TFEx/5lNQa0eKKmw5p4d71cFWYK/img.png|CDM|1.3|{"originWidth":1916,"originHeight":1370,"style":"alignCenter"}_##]

### 테스트 설명

- 주석 1
  - 한번 메시지를 poll 할 때 3개의 메시지를 소비하도록 설정한다
- 주석 2
  - auto commit mode 를 활성화한다
- 주석 3
  - 메시지를 발행한다
  - 3개 단위로 알파벳, 이모지 순서로 발행한다.
- 주석 4
  - 첫번째 poll
  - 3개 단위이므로 알파벳만 소비된다
- 주석 5
  - 두번째 poll
  - 3개 단위이므로 이모지가 소비된다

##### ☑️ expect: 자동 커밋 모드이므로 두 번째 poll() 호출시 이모지 메시지가 소비된다

### 테스트 결과

테스트는 성공한다

[##_Image|kage@njhkl/btstmIRAqxj/2iFmi8BIKJ2Da2Ywz6FBp1/img.png|CDM|1.3|{"originWidth":1196,"originHeight":292,"style":"alignCenter"}_##]

# seek offset 설정

- latest
- ealiest

# 학습 테스트 정리

- ✅ Consumer 는 Record 라는 단위로 카프카 브로커로 메시지를 소비
- ✅ 실제 메시지 소비는 `poll()` 을 통해 수행됨
- ✅ `subscribe()` 를 통해 여러 토픽에 대해 컨슈머 등록을 할 수 있음
