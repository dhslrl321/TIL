[학습 테스트로 배워보는 kafka] 5. 학습 테스트로 카프카 컨슈머(kafka consumer) 알아보기

앞서 우리는 메시지를 카프카 브로커로 발행하는 producer 의 java client 에 대해서 알아보았다.

이번 시간에는 카프카 브로커에 저장된 로그 메시지들을 consume 하는 consumer java client 인 KafkaConsumer 에 대해서 학습테스트를 통해 알아볼 예정이다

지난 시간과 마찬가지로 다음 3가지 테스트를 통해 기본적인 동작에 대해 알아보자

1. KafkaConsumer 를 통한 메시지 소비 테스트
2. 컨슈머의 multi topic consume 테스트

# KafkaConsumer

`KafkaConsumer` 클래스는 지난 Producer 와 마찬가지로 카프카 브로커의 특정 토픽에 해당하는 이벤트 로그 Record 를 consume 하는 역할을 수행한다

Kafka 의 consumer 는 Connection Pooling 과 네트워크 프로토콜을 관리한다.

#### 카프카는 다른 일반적인 메시지 큐와 **다르게** 메시지를 소비하더라도 해당 **메시지를 삭제하지 않는다.**

요즘 많이 사용하는 다른 메시지 큐인 sqs 를 보면, 메시지 ack 를 거치고 다면 DeletionPolicy 를 설정할 수 있는데, 카프카는 삭제가 없으니 관련 삭제 정책이 없다.

그래서 언제든지, 누구든지 topic 에 대한 event log 를 replay 할 수 있다.

이 컨셉이 바로 다른 메시지 미들웨어들과 카프카가 구분되는 중요한 이유라고 볼 수 있다.

## KafkaConsumer 인스턴스 생성하기

생성자의 parameter 로 전달된 key-value 형태의 properties 를 통해서 client configuration 을 등록한다

[##_Image|kage@yPA4e/btsrBmpCwoo/Sxh7mcWyaRMIxw8mQO1Tx1/img.png|CDM|1.3|{"originWidth":1310,"originHeight":552,"style":"alignCenter","width":772,"height":325}_##]

해당 configuration 의 자세한 사항과 특정 설정들은 [confluent.io/kafka config](https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html) 에서 확인할 수 있다

앞선 프로듀서 테스트와 마찬가지로 여러 테스트에서 사용할 수 있도록 Test Hepler 클래스를 통해 쉽게 컨슈머 인스턴스를 생성할 수 있도록 분리하였다

```java
public class KafkaConsumerTestHelper {

  public static KafkaConsumer<String, String> simpleConsumer() {
    Map<String, Object> props = Map.of(
            "bootstrap.servers", "localhost:9092",
            "group.id", "my-consumer",
            "enable.auto.commit", "true",
            "auto.offset.reset", "earliest",
            // kafka 로 소비할 message 의 s/d 클래스 설정
            "key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer",
            "value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"
    );

    return new KafkaConsumer<>(props);
  }
}
```

여기서 눈여겨볼 설정은 `enable.auto.commit` 과 `auto.offset.reset` 인데 이는 추후 컨슈머의 commit 과 offset 파트에서 다시 이야기를 하도록 하겠다. (stay tuned!!)

## 테스트 1. KafkaConsumer 를 통한 메시지 소비 테스트

이번 테스트에서는 발행된 메시지를 KafkaConsumer 를 통해 소비해보도록 하겠다

[##_Image|kage@bRNao2/btsrDIysfeE/c1CGoEuAlxanjfnAo3uMGk/img.png|CDM|1.3|{"originWidth":2232,"originHeight":1336,"style":"alignCenter"}_##]

### 테스트 분석

- 주석 1.
  - 토픽에 메시지를 발행한다
  - consumer 테스트를 쉽게 하기 위해 테스트용 토픽에 메시지를 produce 하는 Helper 클래스가 사용되었다. 자세한 코드는 [github](https://github.com/my-research/kafka/blob/82ea1c688f5b551ce7840c200c650aeee12f9b12/kafka-test-supports/src/main/java/com/github/support/helper/KafkaConsumerTestHelper.java) 에서 확인할 수 있다
- 주석 2.
  - 카프카 컨슈머 인스턴스를 특정 토픽에 subscribe 시킨다.
  - 카프카 컨슈머 인스턴스는 한번에 여러개의 토픽에 subscribe 할 수 있다
- 주석 3.
  - subscribe 가 되었다고 해서 레코드를 계속해서 consume 하는것이 아니다.
  - 실제로 이벤트 로그를 fetch 하는 일은 `poll()` 메서드에서 수행된다
  - 결과로 ConsumerRecords 를 반환한다. 여러개의 토픽에 대해 consume 이 가능하기 때문에 record 자체도 여러개다.
  - timeout 에 대한 Duration 을 2초로 지정한다
- 주석 4.
  - 특정 토픽에 메시지가 잘 소비되었는지 검증하는 단언문이다. 역시 헬퍼 클래스이며 자세한 코드는 [github](https://github.com/my-research/kafka/blob/73d72a7bf4408b4e87f4d735e5c18ec1886a20a5/kafka-test-supports/src/main/java/com/github/support/assertions/KafkaAssertions.java#L11) 에서 확인할 수 있다

## test 2. 컨슈머의 multi topic consume 테스트

앞선 테스트에서 `subscribe()` 메서드를 통해서 여러개의 토픽에 대해 소비를 할 수 있다고 하였다.

실제로 테스트해보자

[##_Image|kage@d7AUbC/btsrDIkVPmw/GvalEdgrK9ySFxg7TEZI61/img.png|CDM|1.3|{"originWidth":2092,"originHeight":1570,"style":"alignCenter"}_##]

helper 클래스를 통해 2개의 서로 다른 토픽에 메시지를 수행했다.

KafkaConsumer 인스턴스에 서로 다른 앞선 2가지 토픽을 subscribe 하였고 마찬가지로 `poll()` 을 수행했다.

그리고 Records 단언을 통해 2개의 메시지를 소비하는지 확인하였고, 성공하였다

# kafka 의 offset 과 commit

앞선 여러 시간에 걸쳐서 계속해서 강조해온 중요한 내용이 있다.

#### 카프카는 애플리케이션에서 발생하는 이벤트에 대한 로그 시스템이다.

로그는 시스템 내에서 일어난 **일련의 사건들**이다.

우리는 일반적으로 발생한 로그들 사이에 어떤 새로운 로그를 추가하려하지 않는다. **단지 로그는 계속해서 쌓일 뿐이다.**

또한 우리는 **특정 시점에 발생한 로그들을 언제든 다시 조회**할 수 있다.

카프카는 로그의 **append only** 와 **seeking by offset** 특성을 따르는 `log based architecture` 를 지향한다.

앞에서도 설명했듯이 카프카는 메시지를 소비하면, 다른 MQ 와 달리 메시지를 삭제(pop) 하지 않는다.

그럼 consumer 는 도대체 어떻게 자신이 가져가야할 message 에 대해서 알 수 있을까?

정답은 바로 offset 과 commit 에 있다.

# offset 과 commit

- kafka 에서 offset 은 토픽에 존재하는 파티션의 특정 메시지(이벤트 로그)의 위치(position)다.
- 카프카는 컨슈머가 메시지를 어디까지 가져갔는지 offset 을 통해서 알 수 있다.
- 컨슈머가 가져간 메시지의 위치 정보(offset) 을 업데이트 (commit) 한다.
  - 이러한 위치 정보는 카프카 내부에서 (`__consumer_offsets`) 라는 토픽을 생성하고 해당 토픽에 컨슈머의 오프셋 정보에 대한 메시지를 저장하는 형태로 구성된다.

# 왜 offset 과 commit 을 두는가?

- 메시지를 중복으로 처리하지 않게 하기 위해서
- 메시지 처리의 정확성을 보장하기 위해서
- 만약 장애 상황이 발생했다면, 복구되었을 때 메시지를 중복해서 소비하지 않도록 하기 위함
  - 마지막으로 commit 한 offset 부터 다시 읽으면 되기 때문에

# commit 의 종류 2가지

- commit 에는 크게 2가지 종류가 존재한다.
  - auto commit
    - 메시지를 가져올 때마다 커밋하는 방법
    - 오프셋 관리하기 가장 쉬운 방법
    - 일정한 주기 혹은 메시지 수를 처리한 후에 자동으로 offset 을 commit
    - `poll()` 을 호출할 때 가장 마지막 오프셋을 커밋함
    - `auto.commit.interval.ms` 설정값으로 조절 가능 기본으로 5초 주기
    - 메시지 처리중 장애가 발생하면 메시지가 중복해서 처리될 가능성이 있음
  - manual commit
    - 메시지 처리가 완료될 때까지 메시지를 가져온 것으로 간주하지 않는 경우에 사용됨
    - 컨슈머가 메시지 처리 후 명시적으로 offset 을 commit 함
      - 예로 db tx 로 묶어서 처리할 수 있음, db 에 저장되고 나서 commit 하는 방법
    - 메시지 처리의 완료/성공 여부에 따라서 offset 을 commit 하니까 중복 처리를 최소화 함
    - 장점: 메시지 처리의 정확성과 데이터 무손실을 보장함

# 학습 테스트 정리

- ✅ Consumer 는 Record 라는 단위로 카프카 브로커로 메시지를 소비
- ✅ 실제 메시지 소비는 `poll()` 을 통해 수행됨
- ✅ `subscribe()` 를 통해 여러 토픽에 대해 컨슈머 등록을 할 수 있음
