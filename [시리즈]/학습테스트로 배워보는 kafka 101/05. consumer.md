[학습 테스트로 배워보는 kafka] 5. 학습 테스트로 카프카 컨슈머(kafka consumer)와 offset 알아보기

---

> 해당 시리즈에서 제공하는 모든 소스코드는 [github repository](https://github.com/my-research/kafka) 에서 제공됩니다. 자세한 코드와 테스트 케이스는 해당 링크에서 확인해주세요.

[##_Image|kage@IlFlY/btstfmH4M7E/EWNC3VTHtHNX9FPKfWbsJ0/img.png|CDM|1.3|{"originWidth":966,"originHeight":486,"style":"alignCenter","width":538,"height":271}_##]

이번 [학습 테스트로 배워보는 kafka] 시리즈는 아래 **순서대로 챕터가 구성**되고, 시리즈 외로 kafka 관련하여 **더욱 많은 학습 정보**는 [kafka 심화 세션](https://wonit.tistory.com/category/%F0%9F%94%AC%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/-%20Event-Driven-Architecture) 에서 확인할 수 있습니다.

### 시리즈 목차

1. [시리즈를 시작하며](#)
2. [kafka 빠르게 훑어보고 아는체하기](#) <-- **현재 글**
3. [kafka 컨셉과 용어 정리](#)
4. [학습테스트 준비하기](#)
5. [학습 테스트로 kafka producer 알아보기](#)
6. [학습 테스트로 kafka consumer 알아보기](#)
7. [학습 테스트로 partitioning 알아보기](#)
8. [학습 테스트로 consumer group 과 rebalancing 알아보기](#)
9. [시리즈를 마치며](#)

_학습의 단계별 순서로 목차가 구성되어있으므로 선행되어야 하는 챕터가 존재합니다_

---

앞서 우리는 메시지를 카프카 브로커로 발행하는 **producer 의 java client** 에 대해서 알아보았다.

이번 시간에는 카프카 브로커에 저장된 메시지들을 **소비(consume) 하는 consumer java client** 인 `KafkaConsumer` 에 대해서 학습테스트를 통해 알아볼 예정이다

# KafkaConsumer

[##_Image|kage@b0BFuJ/btstqBjmRcn/P8VTsEapd03myrqZvt7B91/img.png|CDM|1.3|{"originWidth":3246,"originHeight":1834,"style":"alignCenter"}_##]

`KafkaConsumer` 클래스는 지난 Producer 와 반대의 일을 수행한다.

producer 가 브로커로 메시지를 저장했다면, consumer 가 브로커에 저장된 메시지를 읽는 것이다.

## KafkaConsumer 인스턴스 생성하기

생성자의 parameter 로 전달된 key-value 형태의 properties 를 통해서 client configuration 을 등록한다

[##_Image|kage@yPA4e/btsrBmpCwoo/Sxh7mcWyaRMIxw8mQO1Tx1/img.png|CDM|1.3|{"originWidth":1310,"originHeight":552,"style":"alignCenter","width":772,"height":325}_##]

해당 configuration 의 자세한 사항과 특정 설정들은 [confluent.io/kafka config](https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html) 에서 확인할 수 있다

앞선 프로듀서 테스트와 마찬가지로 여러 테스트에서 사용할 수 있도록 Test Hepler 클래스를 통해 쉽게 컨슈머 인스턴스를 생성할 수 있도록 분리하였다

```java
public class KafkaConsumerTestHelper {

  public static KafkaConsumer<String, String> simpleConsumer() {
    Map<String, Object> props = Map.of(
            "bootstrap.servers", "localhost:9092",
            "group.id", "my-consumer",
            "enable.auto.commit", "true",
            "auto.offset.reset", "earliest",
            // kafka 로 소비할 message 의 s/d 클래스 설정
            "key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer",
            "value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"
    );

    return new KafkaConsumer<>(props);
  }
}
```

여기서 눈여겨볼 설정은 `enable.auto.commit` 과 `auto.offset.reset` 인데 이는 추후 컨슈머의 commit 과 offset 파트에서 다시 이야기를 하도록 하겠다. (stay tuned!!)

## 테스트 1. KafkaConsumer 를 통한 메시지 소비 테스트

이번 테스트에서는 발행된 메시지를 KafkaConsumer 를 통해 소비해보도록 하겠다

[##_Image|kage@bRNao2/btsrDIysfeE/c1CGoEuAlxanjfnAo3uMGk/img.png|CDM|1.3|{"originWidth":2232,"originHeight":1336,"style":"alignCenter"}_##]

### 테스트 분석

- 주석 1.
  - 토픽에 메시지를 발행한다
  - consumer 테스트를 쉽게 하기 위해 테스트용 토픽에 메시지를 produce 하는 Helper 클래스가 사용되었다. 자세한 코드는 [github](https://github.com/my-research/kafka/blob/82ea1c688f5b551ce7840c200c650aeee12f9b12/kafka-test-supports/src/main/java/com/github/support/helper/KafkaConsumerTestHelper.java) 에서 확인할 수 있다
- 주석 2.
  - 카프카 컨슈머 인스턴스를 특정 토픽에 subscribe 시킨다.
  - 카프카 컨슈머 인스턴스는 한번에 여러개의 토픽에 subscribe 할 수 있다
- 주석 3.
  - subscribe 가 되었다고 해서 레코드를 계속해서 consume 하는것이 아니다.
  - 실제로 이벤트 로그를 fetch 하는 일은 `poll()` 메서드에서 수행된다
  - 결과로 ConsumerRecords 를 반환한다. 여러개의 토픽에 대해 consume 이 가능하기 때문에 record 자체도 여러개다.
  - timeout 에 대한 Duration 을 2초로 지정한다
- 주석 4.
  - 특정 토픽에 메시지가 잘 소비되었는지 검증하는 단언문이다. 역시 헬퍼 클래스이며 자세한 코드는 [github](https://github.com/my-research/kafka/blob/73d72a7bf4408b4e87f4d735e5c18ec1886a20a5/kafka-test-supports/src/main/java/com/github/support/assertions/KafkaAssertions.java#L11) 에서 확인할 수 있다

## test 2. 컨슈머의 multi topic consume 테스트

앞선 테스트에서 `subscribe()` 메서드를 통해서 여러개의 토픽에 대해 소비를 할 수 있다고 하였다.

실제로 테스트해보자

[##_Image|kage@d7AUbC/btsrDIkVPmw/GvalEdgrK9ySFxg7TEZI61/img.png|CDM|1.3|{"originWidth":2092,"originHeight":1570,"style":"alignCenter"}_##]

### 테스트 분석

- 주석 1.
  - 서로 다른 topic 에 ("my-topic-1", "my-topic-2") 메시지를 발행한다.
- 주석 2.
  - 하나의 컨슈머가 두개의 토픽을 subscribe 한다
- 주석 3.
  - 두개의 토픽 메시지를 소비하는 것을 확인할 수 있다

# kafka 의 offset 과 commit

카프카라고 한다면 아주 중요한 특성 하나가 존재한다.

#### 카프카는 다른 일반적인 메시지 큐와 **다르게** 메시지를 소비하더라도 해당 **메시지를 삭제하지 않는다.**

[AWS 의 SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html) 처럼 많은 상용 MQ 들을 보면, 메시지를 소비하고 `ack` 라는 과정을 통해 **메시지를 pop** 하여 다음 **메시지를 큐의 제일 끝단에 위치시키는게** 일반적이다.

하지만 kafka 는 그렇지 않다.

kafka 는 **offset** 을 통해 브로커 토픽에 저장된 메시지를 어디까지 읽었는지 확인하고 offset 을 계속해서 옮겨준다.

그래서 언제든지, 다른 컨슈머가 붙더라도 topic 에 대한 메시지를 **replay 할 수 있다**.

이 컨셉이 바로 다른 메시지 미들웨어들과 카프카가 구분되는 중요한 이유라고 볼 수 있다.

# offset 과 commit

- kafka 에서 offset 은 토픽에 존재하는 파티션의 특정 메시지(이벤트 로그)의 위치(position)다.
- 카프카는 컨슈머가 메시지를 어디까지 가져갔는지 offset 을 통해서 알 수 있다.
- 컨슈머가 가져간 메시지의 위치 정보(offset) 을 업데이트 (commit) 한다.
  - 이러한 위치 정보는 카프카 내부에서 (`__consumer_offsets`) 라는 토픽을 생성하고 해당 토픽에 컨슈머의 오프셋 정보에 대한 메시지를 저장하는 형태로 구성된다.

# 왜 offset 과 commit 을 두는가?

- 메시지를 중복으로 처리하지 않게 하기 위해서
- 메시지 처리의 정확성을 보장하기 위해서
- 만약 장애 상황이 발생했다면, 복구되었을 때 메시지를 중복해서 소비하지 않도록 하기 위함
  - 마지막으로 commit 한 offset 부터 다시 읽으면 되기 때문에

# commit 의 종류 2가지

- commit 에는 크게 2가지 종류가 존재한다.
  - auto commit
    - 메시지를 가져올 때마다 커밋하는 방법
    - 오프셋 관리하기 가장 쉬운 방법
    - 일정한 주기 혹은 메시지 수를 처리한 후에 자동으로 offset 을 commit
    - `poll()` 을 호출할 때 가장 마지막 오프셋을 커밋함
    - `auto.commit.interval.ms` 설정값으로 조절 가능 기본으로 5초 주기
    - 메시지 처리중 장애가 발생하면 메시지가 중복해서 처리될 가능성이 있음
  - manual commit
    - 메시지 처리가 완료될 때까지 메시지를 가져온 것으로 간주하지 않는 경우에 사용됨
    - 컨슈머가 메시지 처리 후 명시적으로 offset 을 commit 함
      - 예로 db tx 로 묶어서 처리할 수 있음, db 에 저장되고 나서 commit 하는 방법
    - 메시지 처리의 완료/성공 여부에 따라서 offset 을 commit 하니까 중복 처리를 최소화 함
    - 장점: 메시지 처리의 정확성과 데이터 무손실을 보장함

# seek offset 설정

- latest
- ealiest

# 학습 테스트 정리

- ✅ Consumer 는 Record 라는 단위로 카프카 브로커로 메시지를 소비
- ✅ 실제 메시지 소비는 `poll()` 을 통해 수행됨
- ✅ `subscribe()` 를 통해 여러 토픽에 대해 컨슈머 등록을 할 수 있음
